{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Everything else, [pytest](https://docs.pytest.org/en/latest/) FTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## pytest\n",
    "\n",
    "> The [pytest](https://docs.pytest.org/en/latest/) framework makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## pytest\n",
    "\n",
    "\n",
    "- Tests without boilerplate (no need for `self.assert*` or classes)\n",
    "\n",
    "- Very powerful\n",
    "\n",
    "- Rich plugin architecture\n",
    "\n",
    "- Can run unittests, nosetests, doctests, works well with hypothesis\n",
    "\n",
    "- Mature\n",
    "\n",
    "---\n",
    "\n",
    "*We will only take a look at a few features.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_sample.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_sample.py\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def test_inc():\n",
    "    assert inc(3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /home/claus/repo/PYCONSK18_TESTING, inifile:\n",
      "plugins: hypothesis-3.48.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_sample.py F\u001b[36m                                                  [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_inc ___________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_inc():\u001b[0m\n",
      "\u001b[1m>       assert inc(3) == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 4 == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 4 = inc(3)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_sample.py\u001b[0m:6: AssertionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.04 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Good to Know and Do\n",
    "\n",
    "#### [Fixtures](https://docs.pytest.org/en/latest/fixture.html#fixtures)\n",
    "\n",
    "> The purpose of test fixtures is to provide a fixed baseline upon which tests can reliably and repeatedly execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_fixture.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_fixture.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def data():\n",
    "    return 'We need that'\n",
    "\n",
    "def test_needs(data):\n",
    "    assert data == 'Do we need that?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /home/claus/repo/PYCONSK18_TESTING, inifile:\n",
      "plugins: hypothesis-3.48.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_fixture.py F\u001b[36m                                                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_needs __________________________________\u001b[0m\n",
      "\n",
      "data = 'We need that'\n",
      "\n",
      "\u001b[1m    def test_needs(data):\u001b[0m\n",
      "\u001b[1m>       assert data == 'Do we need that?'\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'We need that' == 'Do we need that?'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         - We need that\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ? ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + Do we need that?\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ? ^^^^           +\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_fixture.py\u001b[0m:8: AssertionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.04 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Asserting Exceptions](https://docs.pytest.org/en/latest/assert.html#assertions-about-expected-exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_exception.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_exception.py\n",
    "import pytest\n",
    "\n",
    "def fun():\n",
    "    raise ZeroDivisionError()\n",
    "\n",
    "def test_fun_raises_zero_division_error():\n",
    "    with pytest.raises(TypeError):\n",
    "        fun()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /home/claus/repo/PYCONSK18_TESTING, inifile:\n",
      "plugins: hypothesis-3.48.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_exception.py F\u001b[36m                                               [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________ test_fun_raises_zero_division_error ______________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_fun_raises_zero_division_error():\u001b[0m\n",
      "\u001b[1m        with pytest.raises(TypeError):\u001b[0m\n",
      "\u001b[1m>           fun()\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_exception.py\u001b[0m:8: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\u001b[1m    def fun():\u001b[0m\n",
      "\u001b[1m>       raise ZeroDivisionError()\u001b[0m\n",
      "\u001b[1m\u001b[31mE       ZeroDivisionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_exception.py\u001b[0m:4: ZeroDivisionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.05 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Parametrizing Test Functions](https://docs.pytest.org/en/latest/parametrize.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_parametrize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_parametrize.py\n",
    "import pytest\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "@pytest.mark.parametrize('x, expected', [\n",
    "    (1, 2),\n",
    "    (4, 5),\n",
    "    (10, 101),\n",
    "])\n",
    "def test_inc(x, expected):\n",
    "    assert inc(x) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /home/claus/repo/PYCONSK18_TESTING, inifile:\n",
      "plugins: hypothesis-3.48.1\n",
      "collected 3 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_parametrize.py ..F\u001b[36m                                           [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_inc[10-101] _______________________________\u001b[0m\n",
      "\n",
      "x = 10, expected = 101\n",
      "\n",
      "\u001b[1m    @pytest.mark.parametrize('x, expected', [\u001b[0m\n",
      "\u001b[1m        (1, 2),\u001b[0m\n",
      "\u001b[1m        (4, 5),\u001b[0m\n",
      "\u001b[1m        (10, 101),\u001b[0m\n",
      "\u001b[1m    ])\u001b[0m\n",
      "\u001b[1m    def test_inc(x, expected):\u001b[0m\n",
      "\u001b[1m>       assert inc(x) == expected\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 11 == 101\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 11 = inc(10)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_parametrize.py\u001b[0m:12: AssertionError\n",
      "\u001b[31m\u001b[1m====================== 1 failed, 2 passed in 0.04 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_parametrize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Temporary Directories and Files](https://docs.pytest.org/en/latest/tmpdir.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_tmpdir.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_tmpdir.py\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def dataset():\n",
    "    # this is some test data we need\n",
    "    df = pd.DataFrame({'a': [1, 2], \n",
    "                       'b': [2, 3]})\n",
    "    return df\n",
    "\n",
    "@pytest.fixture\n",
    "def datafile(tmpdir, dataset):\n",
    "    df = dataset\n",
    "    path = tmpdir.join('df.csv')\n",
    "    df.to_csv(path, index=None)\n",
    "    return path\n",
    "\n",
    "def test_with_tmpdir_fixture(datafile, dataset):\n",
    "    pd.testing.assert_frame_equal(pd.read_csv(datafile), dataset)\n",
    "    assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /home/claus/repo/PYCONSK18_TESTING, inifile:\n",
      "plugins: hypothesis-3.48.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_tmpdir.py F\u001b[36m                                                  [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________ test_with_tmpdir_fixture ___________________________\u001b[0m\n",
      "\n",
      "datafile = local('/tmp/pytest-of-claus/pytest-0/test_with_tmpdir_fixture0/df.csv')\n",
      "dataset =    a  b\n",
      "0  1  2\n",
      "1  2  3\n",
      "\n",
      "\u001b[1m    def test_with_tmpdir_fixture(datafile, dataset):\u001b[0m\n",
      "\u001b[1m        pd.testing.assert_frame_equal(pd.read_csv(datafile), dataset)\u001b[0m\n",
      "\u001b[1m>       assert 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 0\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_tmpdir.py\u001b[0m:21: AssertionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.40 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_tmpdir.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises\n",
    "\n",
    "Implement/fix below (py)tests.\n",
    "\n",
    "---\n",
    "\n",
    "*You do not have to implement all requirements. Pick the ones you find most interesting!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/exercises.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/exercises.py\n",
    "\"\"\"\n",
    "Complete/fix these tests.\n",
    "\"\"\"\n",
    "import pytest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def test_comparing_list():\n",
    "    assert [1, 2, 3] == [1, 3]\n",
    "\n",
    "    \n",
    "def test_comparing_dict():\n",
    "    assert {'a': 1, 'b': 2} == {'c': 2, 'a': 1}\n",
    "\n",
    "    \n",
    "def test_comparing_numbers():\n",
    "    assert (0.1 + 0.1 + 0.1) / 3 == 0.1\n",
    "\n",
    "    \n",
    "def test_comparing_numpy_arrays():\n",
    "    assert np.array([1, 2, 3.1]) == np.array([1, 2, 3])\n",
    "\n",
    "    \n",
    "def test_comparing_dataframes():\n",
    "    assert pd.DataFrame({'a': [1, 2], 'b': [3, 4]}) == pd.DataFrame({'b': [3., 4.], 'a': [1, 2]})\n",
    "\n",
    "    \n",
    "def some_exception():\n",
    "    raise ZeroDivisionError('O.o')\n",
    "    \n",
    "    \n",
    "def test_some_exception():\n",
    "    # check that ZeroDivisionError is raised with 'O.o'\n",
    "    pass\n",
    "\n",
    "\n",
    "def multiply(a, b):\n",
    "    \"\"\"\n",
    "    >>> multiply(1, 2)\n",
    "    2\n",
    "    >>> multiply([3], 3)\n",
    "    [3, 3, 3]\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def test_multiply():\n",
    "    # run doctest using pytest\n",
    "    # make parametrized test case based on doctests\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pytest.main([__file__])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /home/claus/repo/PYCONSK18_TESTING, inifile:\n",
      "plugins: hypothesis-3.48.1\n",
      "collected 7 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/exercises.py FFFFF..\u001b[36m                                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________________ test_comparing_list ______________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_comparing_list():\u001b[0m\n",
      "\u001b[1m>       assert [1, 2, 3] == [1, 3]\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert [1, 2, 3] == [1, 3]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 1 diff: 2 != 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Left contains more items, first extra item: 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/exercises.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m_____________________________ test_comparing_dict ______________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_comparing_dict():\u001b[0m\n",
      "\u001b[1m>       assert {'a': 1, 'b': 2} == {'c': 2, 'a': 1}\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert {'a': 1, 'b': 2} == {'a': 1, 'c': 2}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 1 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Left contains more items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'b': 2}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Right contains more items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'c': 2}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/exercises.py\u001b[0m:15: AssertionError\n",
      "\u001b[31m\u001b[1m____________________________ test_comparing_numbers ____________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_comparing_numbers():\u001b[0m\n",
      "\u001b[1m>       assert (0.1 + 0.1 + 0.1) / 3 == 0.1\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert (((0.1 + 0.1) + 0.1) / 3) == 0.1\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/exercises.py\u001b[0m:19: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________ test_comparing_numpy_arrays __________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_comparing_numpy_arrays():\u001b[0m\n",
      "\u001b[1m>       assert np.array([1, 2, 3.1]) == np.array([1, 2, 3])\u001b[0m\n",
      "\u001b[1m\u001b[31mE       ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/exercises.py\u001b[0m:23: ValueError\n",
      "\u001b[31m\u001b[1m__________________________ test_comparing_dataframes ___________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_comparing_dataframes():\u001b[0m\n",
      "\u001b[1m>       assert pd.DataFrame({'a': [1, 2], 'b': [3, 4]}) == pd.DataFrame({'b': [3., 4.], 'a': [1, 2]})\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/exercises.py\u001b[0m:27: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self =       a     b\n",
      "0  True  True\n",
      "1  True  True\n",
      "\n",
      "\u001b[1m    def __nonzero__(self):\u001b[0m\n",
      "\u001b[1m        raise ValueError(\"The truth value of a {0} is ambiguous. \"\u001b[0m\n",
      "\u001b[1m                         \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\n",
      "\u001b[1m>                        .format(self.__class__.__name__))\u001b[0m\n",
      "\u001b[1m\u001b[31mE       ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/envs/testing/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m:1121: ValueError\n",
      "\u001b[31m\u001b[1m====================== 5 failed, 2 passed in 0.22 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python pytest/exercises.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/solutions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/solutions.py\n",
    "\"\"\"\n",
    "Fix this test file.\n",
    "\"\"\"\n",
    "import pytest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def test_comparing_list():\n",
    "    # assert [1, 2, 3] == [1, 3]\n",
    "    assert [1, 2, 3] == [1, 2, 3]\n",
    "    \n",
    "    \n",
    "def test_comparing_dict():\n",
    "    # dict is unordered, appearance may change (prior to Python 3.6)\n",
    "    # assert {'a': 1, 'b': 2} == {'c': 2, 'a': 1}\n",
    "    assert {'a': 1, 'b': 2} == {'a': 1, 'b': 2}\n",
    "\n",
    "\n",
    "def test_comparing_numbers():\n",
    "    # assert (0.1 + 0.1 + 0.1) / 3 == 0.1\n",
    "    assert (0.1 + 0.1 + 0.1) / 3 == pytest.approx(0.1)\n",
    "    \n",
    "    \n",
    "def test_comparing_numpy_arrays():\n",
    "    # use numpy.testing.assert_allclose\n",
    "    # assert np.array([1, 2, 3.1]) == np.array([1, 2, 3])\n",
    "    np.testing.assert_allclose(np.array([1, 2, 3.1]), np.array([1, 2, 3]), atol=0.1)\n",
    "    \n",
    "    \n",
    "def test_comparing_dataframes():\n",
    "    # use pandas.testing.assert_frame_equal\n",
    "    # assert pd.DataFrame({'a': [1, 2], 'b': [3, 4]}) == pd.DataFrame({'b': [3., 4.], 'a': [1, 2]})\n",
    "    pd.testing.assert_frame_equal(pd.DataFrame({'a': [1, 2], 'b': [3, 4]}), \n",
    "                                  pd.DataFrame({'b': [3., 4.], 'a': [1, 2]}), \n",
    "                                  check_dtype=False)\n",
    "\n",
    "    \n",
    "def some_exception():\n",
    "    raise ZeroDivisionError('O.o')\n",
    "    \n",
    "    \n",
    "def test_some_exception():\n",
    "    with pytest.raises(ZeroDivisionError) as excinfo:\n",
    "        some_exception()\n",
    "    excinfo.value.args[0] == 'O.o'\n",
    "\n",
    "\n",
    "def multiply(a, b):\n",
    "    \"\"\"\n",
    "    >>> multiply(1, 2)\n",
    "    2\n",
    "    >>> multiply([3], 3)\n",
    "    [3, 3, 3]\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"a_b, expected\", [\n",
    "    ((1, 2), 2),\n",
    "    (([3], 3), [3, 3, 3]),\n",
    "])\n",
    "def test_multiply(a_b, expected):\n",
    "    assert multiply(*a_b) == expected\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pytest.main([__file__, '--doctest-modules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /home/claus/repo/PYCONSK18_TESTING, inifile:\n",
      "plugins: hypothesis-3.48.1\n",
      "collected 8 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/solutions.py ........\u001b[36m                                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 8 passed in 0.29 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- pytest is worth learning.\n",
    "\n",
    "\n",
    "- Often, most test cases are rather simple, pytest is great for that.\n",
    "\n",
    "\n",
    "- Sometimes, test cases can be very complex, pytest is great for that too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
